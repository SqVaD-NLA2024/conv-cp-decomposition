{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ai_n_zag@lab.graphicon.ru/temp/conv-cp-decomposition\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from typing import Union\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "from statistics import mean\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.models import alexnet, AlexNet_Weights\n",
    "\n",
    "from conv_cp.conv_cp import decompose_model\n",
    "from conv_cp.imagenet.dataset import ImageNet\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_fn_1(y_pred: torch.Tensor, y_true: torch.Tensor) -> float:\n",
    "    y_pred = y_pred.argmax(dim=1)\n",
    "    correct = (y_pred == y_true).sum().item()\n",
    "    return correct / y_true.size(0)\n",
    "\n",
    "\n",
    "def acc_fn_5(y_pred: torch.Tensor, y_true: torch.Tensor) -> float:\n",
    "    y_pred = y_pred.topk(5, dim=1).indices\n",
    "    correct = (y_pred == y_true.unsqueeze(1)).sum().item()\n",
    "    return correct / y_true.size(0)\n",
    "\n",
    "\n",
    "def loss_fn(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: Union[str, torch.device],\n",
    "    verbose: bool = False,\n",
    "    num_steps: int = 50,\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    total_loss = 0\n",
    "    data_iter = iter(dataloader)\n",
    "    loop = range(num_steps)\n",
    "    if verbose:\n",
    "        loop = tqdm(range(num_steps), desc=\"Validation\")\n",
    "    for step in loop:\n",
    "        try:\n",
    "            x, y = next(data_iter)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x)\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        if verbose:\n",
    "            loop.set_postfix(loss=total_loss / (step + 1))\n",
    "\n",
    "    model.cpu()\n",
    "    return total_loss / num_steps\n",
    "\n",
    "\n",
    "def train_fn(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: Union[str, torch.device],\n",
    "    lr: float,\n",
    "    num_steps: int = 1000,\n",
    "    metric_len: int = 20,\n",
    "    loss_tol: float = 1e-3,\n",
    "    acc_tol: float = 0.95,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    running_loss = deque(maxlen=metric_len)\n",
    "    running_acc = deque(maxlen=metric_len)\n",
    "    runngin_acc_5 = deque(maxlen=metric_len)\n",
    "\n",
    "    data_iter = iter(dataloader)\n",
    "    loop = range(num_steps)\n",
    "    if verbose:\n",
    "        loop = tqdm(range(num_steps), desc=\"Training\")\n",
    "    for _ in loop:\n",
    "        try:\n",
    "            x, y = next(data_iter)\n",
    "        except StopIteration:\n",
    "            data_iter = iter(dataloader)\n",
    "            x, y = next(data_iter)\n",
    "\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss.append(loss.item())\n",
    "        running_acc.append(acc_fn_1(y_pred, y))\n",
    "        runngin_acc_5.append(acc_fn_5(y_pred, y))\n",
    "\n",
    "        if verbose:\n",
    "            loop.set_postfix(\n",
    "                loss=mean(running_loss),\n",
    "                acc_1=mean(running_acc),\n",
    "                acc_5=mean(runngin_acc_5),\n",
    "            )\n",
    "\n",
    "        if mean(running_loss) < loss_tol or mean(running_acc) > acc_tol:\n",
    "            break\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    model.cpu()\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
    "transform = AlexNet_Weights.IMAGENET1K_V1.transforms()\n",
    "\n",
    "dataset = ImageNet(root_dir=\"data/val-images\", transform=transform)\n",
    "train_dataset, val_dataset = dataset.split(0.9)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = partial(\n",
    "    loss_fn,\n",
    "    dataloader=val_loader,\n",
    "    device=\"cuda\",\n",
    "    num_steps=50,\n",
    "    verbose=True,\n",
    ")\n",
    "train_fn = partial(\n",
    "    train_fn,\n",
    "    dataloader=train_loader,\n",
    "    device=\"cuda\",\n",
    "    lr=1e-7,\n",
    "    metric_len=10,\n",
    "    num_steps=100,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing losses for conv layers\n",
      "Using predefined rank 69 for features.0\n",
      "Using predefined rank 154 for features.3\n",
      "Using predefined rank 153 for features.6\n",
      "Using predefined rank 178 for features.8\n",
      "Using predefined rank 196 for features.10\n",
      "Computing losses for fc layers\n",
      "Using predefined rank 365 for classifier.1\n",
      "Using predefined rank 275 for classifier.4\n",
      "Using predefined rank 260 for classifier.6\n",
      "Initializing CP model\n",
      "Decomposing features.0 with rank 69\n",
      "Training model with features.0 decomposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:38<00:00,  2.60it/s, acc_1=0.502, acc_5=0.764, loss=2.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing features.3 with rank 154\n",
      "Training model with features.3 decomposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:39<00:00,  2.52it/s, acc_1=0.52, acc_5=0.747, loss=2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing features.6 with rank 153\n",
      "Training model with features.6 decomposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:38<00:00,  2.61it/s, acc_1=0.468, acc_5=0.726, loss=2.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing features.8 with rank 178\n",
      "Training model with features.8 decomposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:39<00:00,  2.56it/s, acc_1=0.443, acc_5=0.684, loss=2.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing features.10 with rank 196\n",
      "Training model with features.10 decomposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:39<00:00,  2.54it/s, acc_1=0.439, acc_5=0.691, loss=2.56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing classifier.1 with rank 365\n",
      "Training model with classifier.1 decomposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:38<00:00,  2.57it/s, acc_1=0.44, acc_5=0.69, loss=2.53] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing classifier.4 with rank 275\n",
      "Training model with classifier.4 decomposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:38<00:00,  2.61it/s, acc_1=0.446, acc_5=0.709, loss=2.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing classifier.6 with rank 260\n"
     ]
    }
   ],
   "source": [
    "predefined_ranks = {\n",
    "    \"features.0\": 69,\n",
    "    \"features.3\": 154,\n",
    "    \"features.6\": 153,\n",
    "    \"features.8\": 178,\n",
    "    \"features.10\": 196,\n",
    "    \"classifier.1\": 365,\n",
    "    \"classifier.4\": 275,\n",
    "    \"classifier.6\": 260,\n",
    "}\n",
    "\n",
    "cp_model = decompose_model(\n",
    "    model,\n",
    "    conv_rank=750,\n",
    "    fc_rank=900,\n",
    "    loss_fn=loss_fn,\n",
    "    train_fn=train_fn,\n",
    "    trial_rank=5,\n",
    "    layer_size_regularization=0.0,\n",
    "    linear_decomp_type=\"svd\",\n",
    "    freeze_decomposed=False,\n",
    "    verbose=True,\n",
    "    predefined_ranks=predefined_ranks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model: nn.Module, dataloader: DataLoader, device: Union[str, torch.device]\n",
    "):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    accs = []\n",
    "    accs_5 = []\n",
    "    times = []\n",
    "    loop = tqdm(dataloader, desc=\"Evaluation\")\n",
    "    for x, y in loop:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            t_start = time.time()\n",
    "            y_pred = model(x)\n",
    "            times.append(time.time() - t_start)\n",
    "            accs.append(acc_fn_1(y_pred, y))\n",
    "            accs_5.append(acc_fn_5(y_pred, y))\n",
    "            loop.set_postfix(acc=mean(accs), acc_5=mean(accs_5))\n",
    "    return mean(accs), mean(accs_5), sum(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 20/20 [00:08<00:00,  2.32it/s, acc=0.405, acc_5=0.694]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 0.4049402573529412 | Top-5 Accuracy: 0.6939682904411765 | Inference time: 0.058960914611816406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc_1, acc_5, inference_time = evaluate(cp_model, val_loader, \"cuda\")\n",
    "print(f\"Top-1 Accuracy: {acc_1} | Top-5 Accuracy: {acc_5} | Inference time: {inference_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 20/20 [00:08<00:00,  2.38it/s, acc=0.566, acc_5=0.791]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 0.5660960477941176 | Top-5 Accuracy: 0.7906135110294118 | Inference time: 0.4947676658630371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "orig_model = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
    "cp_model.cpu()\n",
    "orig_model.cuda()\n",
    "ref_acc_1, ref_acc_5, ref_inference_time = evaluate(orig_model, val_loader, \"cuda\")\n",
    "print(f\"Top-1 Accuracy: {ref_acc_1} | Top-5 Accuracy: {ref_acc_5} | Inference time: {ref_inference_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig model size: 61100840 | CP model size: 8799651 | Size Ratio: 0.144\n"
     ]
    }
   ],
   "source": [
    "def get_size_ratio(model1: nn.Module, model2: nn.Module) -> float:\n",
    "    size1 = sum(p.numel() for p in model1.parameters())\n",
    "    size2 = sum(p.numel() for p in model2.parameters())\n",
    "    return size1 / size2\n",
    "\n",
    "def get_model_size(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "size_ratio = get_size_ratio(cp_model, orig_model)\n",
    "cp_size = get_model_size(cp_model)\n",
    "orig_size = get_model_size(orig_model)\n",
    "print(f\"Orig model size: {orig_size} | CP model size: {cp_size} | Size Ratio: {size_ratio:.3f}\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conv_cp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
