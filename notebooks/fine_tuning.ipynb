{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nikita/edu/ai-masters/nla1/project\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "from statistics import mean\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.models import alexnet, AlexNet_Weights\n",
    "\n",
    "from conv_cp.conv_cp import decompose_model\n",
    "from conv_cp.imagenet.dataset import ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_1(y_pred: torch.Tensor, y_true: torch.Tensor) -> float:\n",
    "    y_pred = y_pred.argmax(dim=1)\n",
    "    correct = (y_pred == y_true).sum().item()\n",
    "    return correct / y_true.size(0)\n",
    "\n",
    "\n",
    "def acc_5(y_pred: torch.Tensor, y_true: torch.Tensor) -> float:\n",
    "    y_pred = y_pred.topk(5, dim=1).indices\n",
    "    correct = (y_pred == y_true.unsqueeze(1)).sum().item()\n",
    "    return correct / y_true.size(0)\n",
    "\n",
    "\n",
    "def loss_fn(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: Union[str, torch.device],\n",
    "    verbose: bool = False,\n",
    "    num_steps: int = 50,\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    total_loss = 0\n",
    "    data_iter = iter(dataloader)\n",
    "    loop = range(num_steps)\n",
    "    if verbose:\n",
    "        loop = tqdm(range(num_steps), desc=\"Validation\")\n",
    "    for step in loop:\n",
    "        try:\n",
    "            x, y = next(data_iter)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x)\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        if verbose:\n",
    "            loop.set_postfix(loss=total_loss / (step + 1))\n",
    "\n",
    "    model.cpu()\n",
    "    return total_loss / num_steps\n",
    "\n",
    "\n",
    "def train_fn(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: Union[str, torch.device],\n",
    "    lr: float,\n",
    "    num_steps: int = 1000,\n",
    "    metric_len: int = 20,\n",
    "    loss_tol: float = 1e-3,\n",
    "    acc_tol: float = 0.95,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    running_loss = deque(maxlen=metric_len)\n",
    "    running_acc = deque(maxlen=metric_len)\n",
    "    runngin_acc_5 = deque(maxlen=metric_len)\n",
    "\n",
    "    data_iter = iter(dataloader)\n",
    "    loop = range(num_steps)\n",
    "    if verbose:\n",
    "        loop = tqdm(range(num_steps), desc=\"Training\")\n",
    "    for _ in loop:\n",
    "        try:\n",
    "            x, y = next(data_iter)\n",
    "        except StopIteration:\n",
    "            data_iter = iter(dataloader)\n",
    "            x, y = next(data_iter)\n",
    "\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss.append(loss.item())\n",
    "        running_acc.append(acc_1(y_pred, y))\n",
    "        runngin_acc_5.append(acc_5(y_pred, y))\n",
    "\n",
    "        if verbose:\n",
    "            loop.set_postfix(\n",
    "                loss=mean(running_loss),\n",
    "                acc_1=mean(running_acc),\n",
    "                acc_5=mean(runngin_acc_5),\n",
    "            )\n",
    "\n",
    "        if mean(running_loss) < loss_tol or mean(running_acc) > acc_tol:\n",
    "            break\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    model.cpu()\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
    "transform = AlexNet_Weights.IMAGENET1K_V1.transforms()\n",
    "\n",
    "dataset = ImageNet(root_dir=\"data/val-images\", transform=transform)\n",
    "train_dataset, val_dataset = dataset.split(0.9)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = partial(\n",
    "    loss_fn,\n",
    "    dataloader=val_loader,\n",
    "    device=\"cuda\",\n",
    "    num_steps=50,\n",
    "    verbose=True,\n",
    ")\n",
    "train_fn = partial(\n",
    "    train_fn,\n",
    "    dataloader=train_loader,\n",
    "    device=\"cuda\",\n",
    "    lr=1e-7,\n",
    "    metric_len=50,\n",
    "    num_steps=500,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing losses for conv layers\n",
      "Processing module features.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 50/50 [00:08<00:00,  6.01it/s, loss=8.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing module features.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 50/50 [00:07<00:00,  6.56it/s, loss=7.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing module features.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 50/50 [00:07<00:00,  6.44it/s, loss=8.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing module features.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 50/50 [00:07<00:00,  6.38it/s, loss=7.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing module features.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 50/50 [00:07<00:00,  6.32it/s, loss=6.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing losses for fc layers\n",
      "Processing module classifier.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 50/50 [00:08<00:00,  5.61it/s, loss=6.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing module classifier.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 50/50 [00:08<00:00,  6.20it/s, loss=6.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing module classifier.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 50/50 [00:07<00:00,  6.35it/s, loss=5.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CP model\n",
      "Decomposing features.0 with rank 136\n",
      "Training model with features.0 decomposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [01:04<00:00,  7.79it/s, acc_1=0.517, acc_5=0.755, loss=2.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing features.3 with rank 188\n",
      "Training model with features.3 decomposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [01:04<00:00,  7.79it/s, acc_1=0.557, acc_5=0.765, loss=2]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing features.6 with rank 314\n",
      "Training model with features.6 decomposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [01:03<00:00,  7.90it/s, acc_1=0.465, acc_5=0.708, loss=2.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing features.8 with rank 304\n",
      "Training model with features.8 decomposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [01:02<00:00,  8.00it/s, acc_1=0.492, acc_5=0.748, loss=2.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing features.10 with rank 258\n",
      "Training model with features.10 decomposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [01:01<00:00,  8.16it/s, acc_1=0.458, acc_5=0.718, loss=2.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing classifier.1 with rank 786\n",
      "Training model with classifier.1 decomposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [00:55<00:00,  9.03it/s, acc_1=0.465, acc_5=0.723, loss=2.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing classifier.4 with rank 602\n",
      "Training model with classifier.4 decomposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [00:50<00:00,  9.81it/s, acc_1=0.477, acc_5=0.738, loss=2.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing classifier.6 with rank 412\n"
     ]
    }
   ],
   "source": [
    "cp_model = decompose_model(\n",
    "    model,\n",
    "    conv_rank=1200,\n",
    "    fc_rank=1800,\n",
    "    loss_fn=loss_fn,\n",
    "    train_fn=train_fn,\n",
    "    trial_rank=5,\n",
    "    layer_size_regularization=0.6,\n",
    "    linear_decomp_type=\"svd\",\n",
    "    freeze_decomposed=False,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model: nn.Module, dataloader: DataLoader, device: Union[str, torch.device]\n",
    "):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    accs = []\n",
    "    accs_5 = []\n",
    "    loop = tqdm(dataloader, desc=\"Evaluation\")\n",
    "    for x, y in loop:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x)\n",
    "            accs.append(acc_1(y_pred, y))\n",
    "            accs_5.append(acc_5(y_pred, y))\n",
    "            loop.set_postfix(acc=mean(accs), acc_5=mean(accs_5))\n",
    "    return mean(accs), mean(accs_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation:   0%|          | 0/157 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 157/157 [00:32<00:00,  4.78it/s, acc=0.474, acc_5=0.737]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 0.474 | Top-5 Accuracy: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracies = evaluate(cp_model, val_loader, \"cuda\")\n",
    "print(f\"Top-1 Accuracy: {accuracies[0]:.3f} | Top-5 Accuracy: {accuracies[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 157/157 [00:25<00:00,  6.08it/s, acc=0.562, acc_5=0.785]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 0.562 | Top-5 Accuracy: 0.785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ref_model = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
    "cp_model.cpu()\n",
    "ref_model.cuda()\n",
    "accuracies_ref = evaluate(ref_model, val_loader, \"cuda\")\n",
    "print(f\"Top-1 Accuracy: {accuracies_ref[0]:.3f} | Top-5 Accuracy: {accuracies_ref[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size Ratio: 0.296\n"
     ]
    }
   ],
   "source": [
    "def get_size_ratio(model1: nn.Module, model2: nn.Module) -> float:\n",
    "    size1 = sum(p.numel() for p in model1.parameters())\n",
    "    size2 = sum(p.numel() for p in model2.parameters())\n",
    "    return size1 / size2\n",
    "\n",
    "size_ratio = get_size_ratio(cp_model, ref_model)\n",
    "print(f\"Size Ratio: {size_ratio:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
