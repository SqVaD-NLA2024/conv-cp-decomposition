{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nikita/edu/ai-masters/nla1/project\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "from statistics import mean\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.models import alexnet, AlexNet_Weights\n",
    "\n",
    "from conv_cp.conv_cp import decompose_model\n",
    "from conv_cp.imagenet.dataset import ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_1(y_pred: torch.Tensor, y_true: torch.Tensor) -> float:\n",
    "    y_pred = y_pred.argmax(dim=1)\n",
    "    correct = (y_pred == y_true).sum().item()\n",
    "    return correct / y_true.size(0)\n",
    "\n",
    "\n",
    "def acc_5(y_pred: torch.Tensor, y_true: torch.Tensor) -> float:\n",
    "    y_pred = y_pred.topk(5, dim=1).indices\n",
    "    correct = (y_pred == y_true.unsqueeze(1)).sum().item()\n",
    "    return correct / y_true.size(0)\n",
    "\n",
    "\n",
    "def loss_fn(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: Union[str, torch.device],\n",
    "    verbose: bool = False,\n",
    "    num_steps: int = 50,\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    total_loss = 0\n",
    "    data_iter = iter(dataloader)\n",
    "    loop = range(num_steps)\n",
    "    if verbose:\n",
    "        loop = tqdm(range(num_steps), desc=\"Validation\")\n",
    "    for step in loop:\n",
    "        try:\n",
    "            x, y = next(data_iter)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x)\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        if verbose:\n",
    "            loop.set_postfix(loss=total_loss / (step + 1))\n",
    "\n",
    "    model.cpu()\n",
    "    return total_loss / num_steps\n",
    "\n",
    "\n",
    "def train_fn(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: Union[str, torch.device],\n",
    "    lr: float,\n",
    "    num_steps: int = 1000,\n",
    "    metric_len: int = 20,\n",
    "    loss_tol: float = 1e-3,\n",
    "    acc_tol: float = 0.95,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    running_loss = deque(maxlen=metric_len)\n",
    "    running_acc = deque(maxlen=metric_len)\n",
    "    runngin_acc_5 = deque(maxlen=metric_len)\n",
    "\n",
    "    data_iter = iter(dataloader)\n",
    "    loop = range(num_steps)\n",
    "    if verbose:\n",
    "        loop = tqdm(range(num_steps), desc=\"Training\")\n",
    "    for _ in loop:\n",
    "        try:\n",
    "            x, y = next(data_iter)\n",
    "        except StopIteration:\n",
    "            data_iter = iter(dataloader)\n",
    "            x, y = next(data_iter)\n",
    "\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss.append(loss.item())\n",
    "        running_acc.append(acc_1(y_pred, y))\n",
    "        runngin_acc_5.append(acc_5(y_pred, y))\n",
    "\n",
    "        if verbose:\n",
    "            loop.set_postfix(\n",
    "                loss=mean(running_loss),\n",
    "                acc_1=mean(running_acc),\n",
    "                acc_5=mean(runngin_acc_5),\n",
    "            )\n",
    "\n",
    "        if mean(running_loss) < loss_tol or mean(running_acc) > acc_tol:\n",
    "            break\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    model.cpu()\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
    "transform = AlexNet_Weights.IMAGENET1K_V1.transforms()\n",
    "\n",
    "dataset = ImageNet(root_dir=\"data/val-images\", transform=transform)\n",
    "train_dataset, val_dataset = dataset.split(0.9)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = partial(\n",
    "    loss_fn,\n",
    "    dataloader=val_loader,\n",
    "    device=\"cuda\",\n",
    "    num_steps=50,\n",
    "    verbose=True,\n",
    ")\n",
    "train_fn = partial(\n",
    "    train_fn,\n",
    "    dataloader=train_loader,\n",
    "    device=\"cuda\",\n",
    "    lr=1e-6,\n",
    "    metric_len=50,\n",
    "    num_steps=500,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing losses for conv layers\n",
      "Processing module features.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 50/50 [00:08<00:00,  5.93it/s, loss=8.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing module features.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 50/50 [00:06<00:00,  7.24it/s, loss=8.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing module features.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 50/50 [00:07<00:00,  6.92it/s, loss=9.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing module features.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 50/50 [00:07<00:00,  6.99it/s, loss=6.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing module features.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 50/50 [00:07<00:00,  6.76it/s, loss=5.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing losses for fc layers\n",
      "Processing module classifier.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 50/50 [00:07<00:00,  6.32it/s, loss=5.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing module classifier.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 50/50 [00:07<00:00,  6.45it/s, loss=5.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing module classifier.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 50/50 [00:07<00:00,  6.61it/s, loss=4.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CP model\n",
      "Decomposing features.0 with rank 167\n",
      "Training model with features.0 decomposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [01:07<00:00,  7.45it/s, acc_1=0.475, acc_5=0.752, loss=2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing features.3 with rank 164\n",
      "Training model with features.3 decomposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [01:06<00:00,  7.48it/s, acc_1=0.48, acc_5=0.733, loss=2.32] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing features.6 with rank 186\n",
      "Training model with features.6 decomposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [01:05<00:00,  7.58it/s, acc_1=0.448, acc_5=0.718, loss=2.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing features.8 with rank 122\n",
      "Training model with features.8 decomposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [01:04<00:00,  7.80it/s, acc_1=0.445, acc_5=0.693, loss=2.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing features.10 with rank 111\n",
      "Training model with features.10 decomposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [01:03<00:00,  7.83it/s, acc_1=0.438, acc_5=0.677, loss=2.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing classifier.1 with rank 316\n",
      "Decomposing classifier.4 with rank 321\n",
      "Decomposing classifier.6 with rank 263\n"
     ]
    }
   ],
   "source": [
    "cp_model = decompose_model(\n",
    "    model,\n",
    "    conv_rank=750,\n",
    "    fc_rank=900,\n",
    "    loss_fn=loss_fn,\n",
    "    train_fn=train_fn,\n",
    "    trial_rank=15,\n",
    "    linear_decomp_type=\"svd\",\n",
    "    freeze_decomposed=False,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model: nn.Module, dataloader: DataLoader, device: Union[str, torch.device]\n",
    "):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    accs = []\n",
    "    accs_5 = []\n",
    "    loop = tqdm(dataloader, desc=\"Evaluation\")\n",
    "    for x, y in loop:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x)\n",
    "            accs.append(acc_1(y_pred, y))\n",
    "            accs_5.append(acc_5(y_pred, y))\n",
    "            loop.set_postfix(acc=mean(accs), acc_5=mean(accs_5))\n",
    "    return mean(accs), mean(accs_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 157/157 [00:35<00:00,  4.36it/s, acc=0.357, acc_5=0.635]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 0.357 | Top-5 Accuracy: 0.635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracies = evaluate(cp_model, val_loader, \"cuda\")\n",
    "print(f\"Top-1 Accuracy: {accuracies[0]:.3f} | Top-5 Accuracy: {accuracies[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 157/157 [00:23<00:00,  6.55it/s, acc=0.562, acc_5=0.785]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 0.562 | Top-5 Accuracy: 0.785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ref_model = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
    "cp_model.cpu()\n",
    "ref_model.cuda()\n",
    "accuracies_ref = evaluate(ref_model, val_loader, \"cuda\")\n",
    "print(f\"Top-1 Accuracy: {accuracies_ref[0]:.3f} | Top-5 Accuracy: {accuracies_ref[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size Ratio: 0.139\n"
     ]
    }
   ],
   "source": [
    "def get_size_ratio(model1: nn.Module, model2: nn.Module) -> float:\n",
    "    size1 = sum(p.numel() for p in model1.parameters())\n",
    "    size2 = sum(p.numel() for p in model2.parameters())\n",
    "    return size1 / size2\n",
    "\n",
    "size_ratio = get_size_ratio(cp_model, ref_model)\n",
    "print(f\"Size Ratio: {size_ratio:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
