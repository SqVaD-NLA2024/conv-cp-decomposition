{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Union\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "from statistics import mean\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.models import alexnet, AlexNet_Weights\n",
    "\n",
    "from conv_cp.conv_cp import decompose_model\n",
    "from conv_cp.imagenet.dataset import ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_fn_1(y_pred: torch.Tensor, y_true: torch.Tensor) -> float:\n",
    "    y_pred = y_pred.argmax(dim=1)\n",
    "    correct = (y_pred == y_true).sum().item()\n",
    "    return correct / y_true.size(0)\n",
    "\n",
    "\n",
    "def acc_fn_5(y_pred: torch.Tensor, y_true: torch.Tensor) -> float:\n",
    "    y_pred = y_pred.topk(5, dim=1).indices\n",
    "    correct = (y_pred == y_true.unsqueeze(1)).sum().item()\n",
    "    return correct / y_true.size(0)\n",
    "\n",
    "\n",
    "def loss_fn(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: Union[str, torch.device],\n",
    "    verbose: bool = False,\n",
    "    num_steps: int = 50,\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    total_loss = 0\n",
    "    data_iter = iter(dataloader)\n",
    "    loop = range(num_steps)\n",
    "    if verbose:\n",
    "        loop = tqdm(range(num_steps), desc=\"Validation\")\n",
    "    for step in loop:\n",
    "        try:\n",
    "            x, y = next(data_iter)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x)\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        if verbose:\n",
    "            loop.set_postfix(loss=total_loss / (step + 1))\n",
    "\n",
    "    model.cpu()\n",
    "    return total_loss / num_steps\n",
    "\n",
    "\n",
    "def train_fn(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: Union[str, torch.device],\n",
    "    lr: float,\n",
    "    num_steps: int = 1000,\n",
    "    metric_len: int = 20,\n",
    "    loss_tol: float = 1e-3,\n",
    "    acc_tol: float = 0.95,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    running_loss = deque(maxlen=metric_len)\n",
    "    running_acc = deque(maxlen=metric_len)\n",
    "    runngin_acc_5 = deque(maxlen=metric_len)\n",
    "\n",
    "    data_iter = iter(dataloader)\n",
    "    loop = range(num_steps)\n",
    "    if verbose:\n",
    "        loop = tqdm(range(num_steps), desc=\"Training\")\n",
    "    for _ in loop:\n",
    "        try:\n",
    "            x, y = next(data_iter)\n",
    "        except StopIteration:\n",
    "            data_iter = iter(dataloader)\n",
    "            x, y = next(data_iter)\n",
    "\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss.append(loss.item())\n",
    "        running_acc.append(acc_fn_1(y_pred, y))\n",
    "        runngin_acc_5.append(acc_fn_5(y_pred, y))\n",
    "\n",
    "        if verbose:\n",
    "            loop.set_postfix(\n",
    "                loss=mean(running_loss),\n",
    "                acc_1=mean(running_acc),\n",
    "                acc_5=mean(runngin_acc_5),\n",
    "            )\n",
    "\n",
    "        if mean(running_loss) < loss_tol or mean(running_acc) > acc_tol:\n",
    "            break\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    model.cpu()\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
    "transform = AlexNet_Weights.IMAGENET1K_V1.transforms()\n",
    "\n",
    "dataset = ImageNet(root_dir=\"data/val-images\", transform=transform)\n",
    "train_dataset, val_dataset = dataset.split(0.9)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = partial(\n",
    "    loss_fn,\n",
    "    dataloader=val_loader,\n",
    "    device=\"cuda\",\n",
    "    num_steps=20,\n",
    "    verbose=True,\n",
    ")\n",
    "train_fn = partial(\n",
    "    train_fn,\n",
    "    dataloader=train_loader,\n",
    "    device=\"cuda\",\n",
    "    lr=1e-7,\n",
    "    metric_len=10,\n",
    "    num_steps=100,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing losses for conv layers\n",
      "Processing module features.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:08<00:00,  2.43it/s, loss=8.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing module features.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:08<00:00,  2.42it/s, loss=7.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing module features.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:08<00:00,  2.41it/s, loss=8.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing module features.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:08<00:00,  2.39it/s, loss=7.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing module features.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:07<00:00,  2.55it/s, loss=6.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing losses for fc layers\n",
      "Processing module classifier.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:08<00:00,  2.43it/s, loss=6.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing module classifier.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:08<00:00,  2.30it/s, loss=6.77]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing module classifier.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:08<00:00,  2.45it/s, loss=5.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CP model\n",
      "Decomposing features.0 with rank 164\n",
      "Training model with features.0 decomposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:41<00:00,  2.40it/s, acc_1=0.507, acc_5=0.752, loss=2.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing features.3 with rank 147\n",
      "Training model with features.3 decomposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:41<00:00,  2.42it/s, acc_1=0.482, acc_5=0.725, loss=2.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing features.6 with rank 169\n",
      "Training model with features.6 decomposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:41<00:00,  2.43it/s, acc_1=0.464, acc_5=0.714, loss=2.42]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing features.8 with rank 140\n",
      "Training model with features.8 decomposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:41<00:00,  2.42it/s, acc_1=0.42, acc_5=0.681, loss=2.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing features.10 with rank 130\n",
      "Training model with features.10 decomposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:41<00:00,  2.42it/s, acc_1=0.418, acc_5=0.669, loss=2.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing classifier.1 with rank 318\n",
      "Training model with classifier.1 decomposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:41<00:00,  2.43it/s, acc_1=0.434, acc_5=0.687, loss=2.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing classifier.4 with rank 321\n",
      "Training model with classifier.4 decomposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:40<00:00,  2.45it/s, acc_1=0.421, acc_5=0.679, loss=2.64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing classifier.6 with rank 261\n"
     ]
    }
   ],
   "source": [
    "cp_model = decompose_model(\n",
    "    model,\n",
    "    conv_rank=750,\n",
    "    fc_rank=900,\n",
    "    loss_fn=loss_fn,\n",
    "    train_fn=train_fn,\n",
    "    trial_rank=5,\n",
    "    layer_size_regularization=0.0,\n",
    "    linear_decomp_type=\"svd\",\n",
    "    freeze_decomposed=False,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model: nn.Module, dataloader: DataLoader, device: Union[str, torch.device]\n",
    "):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    accs = []\n",
    "    accs_5 = []\n",
    "    times = []\n",
    "    loop = tqdm(dataloader, desc=\"Evaluation\")\n",
    "    for x, y in loop:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            t_start = time.time()\n",
    "            y_pred = model(x)\n",
    "            times.append(time.time() - t_start)\n",
    "            accs.append(acc_fn_1(y_pred, y))\n",
    "            accs_5.append(acc_fn_5(y_pred, y))\n",
    "            loop.set_postfix(acc=mean(accs), acc_5=mean(accs_5))\n",
    "    return mean(accs), mean(accs_5), sum(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 20/20 [00:08<00:00,  2.39it/s, acc=0.395, acc_5=0.679]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 0.3946001838235294 | Top-5 Accuracy: 0.6787913602941177 | Inference time: 0.056990623474121094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc_1, acc_5, inference_time = evaluate(cp_model, val_loader, \"cuda\")\n",
    "print(f\"Top-1 Accuracy: {acc_1} | Top-5 Accuracy: {acc_5} | Inference time: {inference_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 20/20 [00:09<00:00,  2.19it/s, acc=0.566, acc_5=0.791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 0.5660960477941176 | Top-5 Accuracy: 0.7906135110294118 | Inference time: 0.025220155715942383\n"
     ]
    }
   ],
   "source": [
    "orig_model = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
    "cp_model.cpu()\n",
    "orig_model.cuda()\n",
    "ref_acc_1, ref_acc_5, ref_inference_time = evaluate(orig_model, val_loader, \"cuda\")\n",
    "print(f\"Top-1 Accuracy: {ref_acc_1} | Top-5 Accuracy: {ref_acc_5} | Inference time: {ref_inference_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig model size: 61100840 | CP model size: 8513084 | Size Ratio: 0.139\n"
     ]
    }
   ],
   "source": [
    "def get_size_ratio(model1: nn.Module, model2: nn.Module) -> float:\n",
    "    size1 = sum(p.numel() for p in model1.parameters())\n",
    "    size2 = sum(p.numel() for p in model2.parameters())\n",
    "    return size1 / size2\n",
    "\n",
    "def get_model_size(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "size_ratio = get_size_ratio(cp_model, orig_model)\n",
    "cp_size = get_model_size(cp_model)\n",
    "orig_size = get_model_size(orig_model)\n",
    "print(f\"Orig model size: {orig_size} | CP model size: {cp_size} | Size Ratio: {size_ratio:.3f}\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conv_cp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
